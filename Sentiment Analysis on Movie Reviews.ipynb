{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on Movie Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaeqCb5g3PIq"
      },
      "source": [
        "#Sentiment Analysis on Movie Reviews\n",
        "Author: Lufei Ouyang\n",
        "\n",
        "Data set: Pang and Lee Movie Review Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vJeCWc0-dSb"
      },
      "source": [
        "from torchtext.vocab import Vectors\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjj2QARozdkR"
      },
      "source": [
        "### Load the word vector for generating the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j63OTxI-QDS"
      },
      "source": [
        "vec = GloVe(name='6B', dim=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz6-4tHBzuWL"
      },
      "source": [
        "### Read trainning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW5oMfeHqQGj",
        "outputId": "d5899859-fc13-4db8-d723-d6be4bc44bb1"
      },
      "source": [
        "import os\n",
        "#read trainning data\n",
        "neg_train_files = os.listdir('neg')\n",
        "pos_train_files = os.listdir('pos')\n",
        "\n",
        "pos_train_string = ''\n",
        "neg_train_string = ''\n",
        "f_train_pos = None\n",
        "f_train_neg = None\n",
        "i = 0\n",
        "for file in pos_train_files:\n",
        "  if \"txt\" in file and int(file[2:5]) < 600:\n",
        "        i+=1\n",
        "        f_train_pos=open(os.path.join('pos',file),'r')\n",
        "        pos_train_string += f_train_pos.read()\n",
        "        f_train_pos.close()\n",
        "for file in neg_train_files:\n",
        "  if \"txt\" in file and int(file[2:5]) < 600:\n",
        "        f_train_neg=open(os.path.join('neg',file),'r')\n",
        "        neg_train_string += f_train_neg.read()\n",
        "        f_train_neg.close()\n",
        "len(pos_train_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QLD-iyNz09L"
      },
      "source": [
        "### Read test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uovtQ9sA4F8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dbf831-c445-4872-efb4-34e9df88a7a8"
      },
      "source": [
        "#read test data\n",
        "neg_test_files = os.listdir('neg')\n",
        "pos_test_files = os.listdir('pos')\n",
        "pos_test_string = ''\n",
        "neg_test_string = ''\n",
        "f_test_pos = None\n",
        "f_test_neg = None\n",
        "i = 0\n",
        "for file in pos_test_files:\n",
        "  if \"txt\" in file and int(file[2:5]) >= 600:\n",
        "        \n",
        "        f_test_pos=open(os.path.join('pos',file),'r')\n",
        "        pos_test_string += f_test_pos.read()\n",
        "        f_test_pos.close()\n",
        "for file in neg_test_files:\n",
        "  if \"txt\" in file and int(file[2:5]) >= 600:\n",
        "        i+=1\n",
        "        f_test_neg=open(os.path.join('neg',file),'r')\n",
        "        neg_test_string += f_test_neg.read()\n",
        "        f_test_neg.close()\n",
        "len(neg_train_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIhB6wh4z6CN"
      },
      "source": [
        "### Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIoiih9nITd6"
      },
      "source": [
        "#processing train data\n",
        "\n",
        "#split each sentence\n",
        "pos_train_sentences = pos_train_string.split(' \\n')\n",
        "pos_train_words = []\n",
        "pos_train_glove = []\n",
        "\n",
        "#split each word into a list\n",
        "for sentence in pos_train_sentences:\n",
        "  pos_train_words.append(sentence.split(' '))\n",
        "for sentence_words in pos_train_words:\n",
        "  pos_train_glove.append(vec.get_vecs_by_tokens(sentence_words, lower_case_backup=True))\n",
        "#split each sentence\n",
        "neg_train_sentences = neg_train_string.split(' \\n')\n",
        "neg_train_words = []\n",
        "neg_train_glove = []\n",
        "#split each word into a list\n",
        "for sentence in neg_train_sentences:\n",
        "  neg_train_words.append(sentence.split(' '))\n",
        "for sentence_words in neg_train_words:\n",
        "  neg_train_glove.append(vec.get_vecs_by_tokens(sentence_words, lower_case_backup=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJVNo5E2LZJU"
      },
      "source": [
        "#processing test data\n",
        "\n",
        "#split each sentence\n",
        "pos_test_sentences = pos_test_string.split(' \\n')\n",
        "pos_test_words = []\n",
        "pos_test_glove = []\n",
        "#split each word into a list\n",
        "for sentence in pos_test_sentences:\n",
        "  pos_test_words.append(sentence.split(' '))\n",
        "for sentence_words in pos_test_words:\n",
        "  pos_test_glove.append(vec.get_vecs_by_tokens(sentence_words, lower_case_backup=True))\n",
        "#split each sentence\n",
        "neg_test_sentences = neg_test_string.split(' \\n')\n",
        "neg_test_words = []\n",
        "neg_test_glove = []\n",
        "#split each word into a list\n",
        "for sentence in neg_test_sentences:\n",
        "  neg_test_words.append(sentence.split(' '))\n",
        "for sentence_words in neg_test_words:\n",
        "  neg_test_glove.append(vec.get_vecs_by_tokens(sentence_words, lower_case_backup=True))\n",
        "#create the label list\n",
        "label_test = []\n",
        "sentence_test_vec = []\n",
        "for i in range(len(pos_test_glove)):\n",
        "  sentence_test_vec.append(pos_test_glove[i])\n",
        "  label_test.append(torch.IntTensor(1))\n",
        "for i in range(len(neg_test_glove)):\n",
        "  sentence_test_vec.append(neg_test_glove[i])\n",
        "  label_test.append(torch.IntTensor(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YTPlDDZ1L7l"
      },
      "source": [
        "### Convert data into sequence and embedding matrix to make it usable in neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuThnM2mPbBC"
      },
      "source": [
        "#convert list of train data to sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000000)\n",
        "tokenizer.fit_on_texts(sentence_train)\n",
        "word_index = tokenizer.word_index\n",
        "train_sequences = tokenizer.texts_to_sequences(sentence_train)\n",
        "data = pad_sequences(train_sequences, maxlen=100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPK8awGhisSY"
      },
      "source": [
        "#convert list of test data to sequence\n",
        "sentence_test =pos_test_sentences + neg_test_sentences\n",
        "test_sequences = tokenizer.texts_to_sequences(sentence_test)\n",
        "test_data = pad_sequences(test_sequences, maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5oI13sLYF0g"
      },
      "source": [
        "#create train label data \n",
        "label_train = np.zeros(len(sentence_train))\n",
        "\n",
        "for i in range(len(pos_train_sentences)):\n",
        "  label_train[i] = 1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnRFnKOKi8hg"
      },
      "source": [
        "#create test label data \n",
        "label_test = np.zeros(len(sentence_test))\n",
        "for i in range(len(pos_test_sentences)):\n",
        "  label_test[i] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf2mPM7a1XwR"
      },
      "source": [
        "#create embedding matrix\n",
        "sentence_train_vec = np.zeros((len(word_index) + 1, 300))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  sentence_train_vec[i] = vec.get_vecs_by_tokens(word, lower_case_backup=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFCNJa6h1qHw"
      },
      "source": [
        "###Create neural network layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPwsgg4W9djX",
        "outputId": "2b970c3f-a7f8-4020-e6d8-a474c775aee2"
      },
      "source": [
        "#create model and train model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense\n",
        "from keras.layers import MaxPool1D\n",
        "import keras\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index) + 1,300,weights=[sentence_train_vec],input_length=150,trainable=True))\n",
        "\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(50, 150)))\n",
        "model.add(MaxPool1D(pool_size=150))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train the neural network\n",
        "model.fit(data,label_train,epochs=20,batch_size =100 , verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "384/384 [==============================] - 48s 121ms/step - loss: 0.6549 - accuracy: 0.6025\n",
            "Epoch 2/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.4783 - accuracy: 0.7743\n",
            "Epoch 3/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.3181 - accuracy: 0.8578\n",
            "Epoch 4/20\n",
            "384/384 [==============================] - 47s 122ms/step - loss: 0.2215 - accuracy: 0.9047\n",
            "Epoch 5/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.1619 - accuracy: 0.9304\n",
            "Epoch 6/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.1188 - accuracy: 0.9510\n",
            "Epoch 7/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.0912 - accuracy: 0.9629\n",
            "Epoch 8/20\n",
            "384/384 [==============================] - 46s 120ms/step - loss: 0.0663 - accuracy: 0.9752\n",
            "Epoch 9/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.0494 - accuracy: 0.9818\n",
            "Epoch 10/20\n",
            "384/384 [==============================] - 46s 120ms/step - loss: 0.0404 - accuracy: 0.9857\n",
            "Epoch 11/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.0313 - accuracy: 0.9879\n",
            "Epoch 12/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.0275 - accuracy: 0.9902\n",
            "Epoch 13/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.0244 - accuracy: 0.9913\n",
            "Epoch 14/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.0231 - accuracy: 0.9913\n",
            "Epoch 15/20\n",
            "384/384 [==============================] - 46s 121ms/step - loss: 0.0184 - accuracy: 0.9932\n",
            "Epoch 16/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.0158 - accuracy: 0.9938\n",
            "Epoch 17/20\n",
            "384/384 [==============================] - 47s 122ms/step - loss: 0.0179 - accuracy: 0.9929\n",
            "Epoch 18/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.0155 - accuracy: 0.9937\n",
            "Epoch 19/20\n",
            "384/384 [==============================] - 47s 122ms/step - loss: 0.0161 - accuracy: 0.9934\n",
            "Epoch 20/20\n",
            "384/384 [==============================] - 47s 121ms/step - loss: 0.0151 - accuracy: 0.9938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f453009e650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjb-kwI013f5"
      },
      "source": [
        "### Summary of the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyA4XGt0X2Nb",
        "outputId": "65671fa1-4d35-40b4-b633-61c6aed18473"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 150, 300)          10339800  \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 150, 128)          219648    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 10,559,577\n",
            "Trainable params: 10,559,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sivs9y4c19q5"
      },
      "source": [
        "### Evaluate the neural network by using test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImRR8hoJRnvz",
        "outputId": "5195f0eb-8bee-41f1-9c7d-7ba18b80c0a0"
      },
      "source": [
        "#result\n",
        "model.evaluate(test_data, label_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "824/824 [==============================] - 5s 6ms/step - loss: 3.4709 - accuracy: 0.6057\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.47090744972229, 0.6056787371635437]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B93x7Q_iZ1yf"
      },
      "source": [
        "#Project Summary\n",
        "\n",
        "I use the original Pang and Lee set to make my data set. Since Pang and Lee set does not provide test data set, I split the data set into a trainning data set and a test data set.\n",
        "\n",
        "I chose to use torchtext's glove 6d dim300 to generate the embedding matrix. In the part of building my model, I chose to use Keras to build the model. The reason is that the Sequential class of Keras is the easiest way I have found to build a new model. I just need to add different layers and train this model. My layers are embedding layer -> LSTM layer -> max-pooling -> Flatten layer -> Dense layer.\n",
        "\n",
        "First, I read the training data into a list, and then divide it into sentences. Then use keras.preprocessing to add the word vector of each different word to the embedding matrix. Then padding data set. Because we only need to decide whether a sentence is positive or negative. So, I use 0 for negative and 1 for positive to create a label list. Finally, I use the embedding matrix to create the embedding layer and use the sequence of the sentence as the data, and the list containing 0, 1 as the label list for training. After 20 iterations, the accuracy of the model can reach 99.38%. But the accuracy of test data is only 60.57%. \n",
        "\n",
        "I think the reason is that there are many sentences acting as noise components(each review is positive or negative, but not every sentence showing the positive or negative attitude), so the accuracy of the test set is so low.\n"
      ]
    }
  ]
}